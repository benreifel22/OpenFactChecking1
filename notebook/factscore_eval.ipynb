{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dccf994-2390-4372-b942-24dcb94290c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-28 18:46:09,636\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "[nltk_data] Downloading package punkt to /home/yuxiawang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import string\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create an empty Namespace object\n",
    "args = Namespace()\n",
    "\n",
    "# Manually assign values to attributes\n",
    "args.input_path = \"./data/labeled/test.jsonl\"\n",
    "args.model_name = \"retrieval+llama\"\n",
    "args.data_dir = \"./data/\"\n",
    "args.model_dir = \"~/.cache/huggingface/hub\"\n",
    "args.cache_dir = \".cache/factscore/\"\n",
    "args.openai_key = \"./data/openaikey.txt\"\n",
    "args.cost_estimate = \"consider_cache\"\n",
    "args.abstain_detection_type = None\n",
    "args.print_rate_limit_error = True\n",
    "args.batch_size = 256\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.ERROR if args.print_rate_limit_error else logging.CRITICAL)\n",
    "\n",
    "from factscore.abstain_detection import is_response_abstained\n",
    "from factscore.atomic_facts import AtomicFactGenerator\n",
    "# from factscore.clm import CLM\n",
    "# from factscore.npm import NPM\n",
    "from factscore.openai_lm import OpenAIModel\n",
    "from factscore.retrieval import DocDB, Retrieval\n",
    "\n",
    "def register_knowledge_source(args, name=\"enwiki-20230401\", db_path=None, data_path=None):\n",
    "    db, retrieval, npm = {}, {}, {}\n",
    "    if db_path is None:\n",
    "        db_path = os.path.join(args.data_dir, f\"{name}.db\")\n",
    "\n",
    "    if data_path is None:\n",
    "        data_path = os.path.join(args.data_dir, f\"{name}.jsonl\")\n",
    "\n",
    "    cache_path = os.path.join(args.cache_dir, f\"retrieval-{name}.json\")\n",
    "    embed_cache_path = os.path.join(args.cache_dir, f\"retrieval-{name}.pkl\")\n",
    "\n",
    "    db[name] = DocDB(db_path=db_path, data_path=data_path)\n",
    "    retrieval[name] = Retrieval(db[name], cache_path, embed_cache_path, batch_size=args.batch_size, retrieval_type=\"bm25\")\n",
    "    if \"npm\" in args.model_name:\n",
    "        cache_path = os.path.join(args.cache_dir, f\"bm25-{name}.json\")\n",
    "        embed_cache_path = os.path.join(args.cache_dir, f\"bm25-{name}.pkl\")\n",
    "        self.npm[name] = NPM(Retrieval(db[name], cache_path, embed_cache_path, \"bm25\"),\n",
    "                             \"npm-single\",\n",
    "                             cache_file=os.path.join(args.cache_dir, f\"npm-{name}.pkl\"))\n",
    "    return db, retrieval, npm\n",
    "\n",
    "db, retrieval, npm = register_knowledge_source(args, name=\"enwiki-20230401\", db_path=None, data_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5646473-419d-4b96-a266-3849b3234267",
   "metadata": {},
   "source": [
    "### Atomic claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ce2449-37ca-4b56-bcc7-ae5370554340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "af_generator = AtomicFactGenerator(key_path=args.openai_key,\n",
    "                                   demon_dir=os.path.join(args.data_dir, \"demos\"),\n",
    "                                   gpt3_cache_file=os.path.join(args.cache_dir, \"ChatGPT.pkl\"))\n",
    "\n",
    "generation = \"Doug Sheehan is an American actor best known for his role as Ben Galvin in the hit television drama series, Knots Landing. He has also appeared in films such as The Big Easy and The Last Boy Scout, and television shows such as NYPD Blue and Beverly Hills, 90210. He was born in Los Angeles, California on August 24, 1956. Sheehan attended the University of California, Los Angeles, where he received a Bachelor of Arts degree in Theatre Arts. He has been married since 1984 to Lisa Cooper and has two children.\"\n",
    "\n",
    "curr_afs, para_break = af_generator.run(generation)\n",
    "curr_afs = [fact for _, facts in curr_afs for fact in facts]\n",
    "print(len(curr_afs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe71ce8-2bae-4cc5-8cc6-7453e24ef70d",
   "metadata": {},
   "source": [
    "### Evaluate over claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71adea2-c020-4af6-b48f-212b65c21e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 15:03:10,018\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 15:03:10 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 15:03:13 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 04-28 15:03:13 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:13 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:13 selector.py:28] Using FlashAttention backend.\n",
      "INFO 04-28 15:03:14 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:14 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 04-28 15:03:14 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 04-28 15:03:14 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:14 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m WARNING 04-28 15:03:14 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 04-28 15:03:15 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:15 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 15:03:17 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:17 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "INFO 04-28 15:03:19 ray_gpu_executor.py:217] # GPU blocks: 12256, # CPU blocks: 4096\n",
      "INFO 04-28 15:03:20 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-28 15:03:20 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:20 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:20 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m [rank1]:[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayWorkerWrapper pid=180096)\u001b[0m INFO 04-28 15:03:23 model_runner.py:1057] Graph capturing finished in 4 secs.\n",
      "INFO 04-28 15:03:23 model_runner.py:1057] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  9.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  9.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  9.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.74it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.46it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.84it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "atomic_facts = curr_afs\n",
    "topic = \"Doug Sheehan\"\n",
    "knowledge_source = \"enwiki-20230401\"\n",
    "# from factscore.call_llms import LLaMA3\n",
    "# lm = LLaMA3(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "from factscore.clm import LLaMA3\n",
    "lm = LLaMA3(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            cache_file=os.path.join(args.cache_dir, \"inst-llama3-8B.pkl\"))\n",
    "\n",
    "decisions = []\n",
    "prompts = []\n",
    "total_words = 0\n",
    "for atom in atomic_facts:\n",
    "    atom = atom.strip()\n",
    "    if lm:\n",
    "        passages = retrieval[knowledge_source].get_passages(topic, atom, k=5)\n",
    "        # print(atom, passages)\n",
    "        \n",
    "        definition = \"Answer the question about {} based on the given context.\\n\\n\".format(topic)\n",
    "        context = \"\"\n",
    "        for psg_idx, psg in enumerate(reversed(passages)):\n",
    "            context += \"Title: {}\\nText: {}\\n\\n\".format(psg[\"title\"], psg[\"text\"].replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
    "        definition += context.strip()\n",
    "        if not definition[-1] in string.punctuation:\n",
    "            definition += \".\"\n",
    "        prompt = \"{}\\n\\nInput: {} True or False?\\nOutput:\".format(definition.strip(), atom.strip())\n",
    "        # prompts.append(prompt)\n",
    "\n",
    "        output = lm.generate(prompt)\n",
    "        \n",
    "        # when logits are unavailable\n",
    "        generated_answer = output[0].lower()\n",
    "        if \"true\" in generated_answer or \"false\" in generated_answer:\n",
    "            if \"true\" in generated_answer and \"false\" not in generated_answer:\n",
    "                is_supported = True\n",
    "            elif \"false\" in generated_answer and \"true\" not in generated_answer:\n",
    "                is_supported = False\n",
    "            else:\n",
    "                is_supported = generated_answer.index(\"true\") > generated_answer.index(\"false\")\n",
    "        else:\n",
    "            is_supported = all([keyword not in generated_answer.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split() for keyword in [\"not\", \"cannot\", \"unknown\", \"information\"]])\n",
    "    \n",
    "    decisions.append({\"atom\": atom, \"is_supported\": is_supported})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7298ff-4437-43c7-85a2-958b376db629",
   "metadata": {},
   "source": [
    "### Evaluate on factoolqa and FELM-WK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03370171-393d-4210-a39e-923c05146562",
   "metadata": {},
   "source": [
    "#### Get wikipedia title as topic input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd05535-b254-4cd2-ae97-665728171a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuclear power by country\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Dict\n",
    "\n",
    "def search_google_first_wikipedia_title(query: str, num_web_pages: int = 1, timeout : int = 6):\n",
    "    \"\"\"how to get the related wikipedia title as topic input for each claim or prompt? \n",
    "    Google search: prompt + Wikipedia, the first page title is considered to be.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query.\n",
    "        num_web_pages: the number of web pages to request.\n",
    "        save_url: path to save returned urls, such as 'urls.txt'\n",
    "    Returns:\n",
    "        search_results: the first web page title\n",
    "    \"\"\"\n",
    "    query = query.replace(\" \", \"+\")\n",
    "\n",
    "    # set headers: Google returns different web-pages according to agent device\n",
    "    # desktop user-agent\n",
    "    USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0\"\n",
    "    # mobile user-agent\n",
    "    MOBILE_USER_AGENT = \"Mozilla/5.0 (Linux; Android 7.0; SM-G930V Build/NRD90M) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/59.0.3071.125 Mobile Safari/537.36\"\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    \n",
    "    # set language\n",
    "    # set the Google interface language, use &hl=XX\n",
    "    # set the preferred language of the search results, use &lr=lang_XX\n",
    "    # set language as en, otherwise it will return many translation web pages to Arabic that can't be opened correctly.\n",
    "    lang = \"en\" \n",
    "\n",
    "    # scrape google results\n",
    "    urls = []\n",
    "    for page in range(0, num_web_pages, 10):\n",
    "        # here page is google search's bottom page meaning, click 2 -> start=10\n",
    "        # url = \"https://www.google.com/search?q={}&start={}\".format(query, page)\n",
    "        url = \"https://www.google.com/search?q={}&lr=lang_{}&hl={}&start={}\".format(query, lang, lang, page)\n",
    "        r = requests.get(url, headers=headers, timeout=timeout)\n",
    "        # collect all urls by regular expression\n",
    "        # how to do if I just want to have the returned top-k pages?\n",
    "        urls += re.findall('href=\"(https?://.*?)\"', r.text)\n",
    "\n",
    "        # Find the first web page title\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        first_result = soup.find(\"h3\", class_=\"LC20lb MBeuO DKV0Md\")\n",
    "        if first_result:\n",
    "            return first_result.text\n",
    "        else:\n",
    "            print(\"Fail to get the first page title.\")\n",
    "            return r\n",
    "\n",
    "\n",
    "query = \"Which country or city has the maximum number of nuclear power plants? Wikepedia\"\n",
    "a = search_google_first_wikipedia_title(query)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41f233-7cfa-4901-910e-ebeb0db8c345",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3b9286-2ecb-413d-b404-19b24f8c471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../data/Factbench.jsonl\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a4c248-6e87-4112-ac8f-6eb7e277203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>response_label</th>\n",
       "      <th>claims</th>\n",
       "      <th>claim_labels</th>\n",
       "      <th>ability_to_test</th>\n",
       "      <th>source</th>\n",
       "      <th>hallucination_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>What did SOS originally stand for?</td>\n",
       "      <td>SOS originally stood for \"Save Our Souls\" or \"...</td>\n",
       "      <td>False</td>\n",
       "      <td>[SOS originally stood for \"Save Our Souls\", SO...</td>\n",
       "      <td>[False, False, True, True, False, True, True]</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>factool-qa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Which country or city has the maximum number o...</td>\n",
       "      <td>The United States has the highest number of nu...</td>\n",
       "      <td>False</td>\n",
       "      <td>[The United States has the highest number of n...</td>\n",
       "      <td>[False, True]</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>felm-wk</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prompt  \\\n",
       "49                 What did SOS originally stand for?   \n",
       "50  Which country or city has the maximum number o...   \n",
       "\n",
       "                                             response response_label  \\\n",
       "49  SOS originally stood for \"Save Our Souls\" or \"...          False   \n",
       "50  The United States has the highest number of nu...          False   \n",
       "\n",
       "                                               claims  \\\n",
       "49  [SOS originally stood for \"Save Our Souls\", SO...   \n",
       "50  [The United States has the highest number of n...   \n",
       "\n",
       "                                     claim_labels ability_to_test      source  \\\n",
       "49  [False, False, True, True, False, True, True]       knowledge  factool-qa   \n",
       "50                                  [False, True]       knowledge     felm-wk   \n",
       "\n",
       "   hallucination_spans  \n",
       "49                None  \n",
       "50                None  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[49:51]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d4f84c-35d3-487b-9795-c6bda097e6ce",
   "metadata": {},
   "source": [
    "#### load models and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f8c1681-5a8d-4d08-9522-23dc24d65085",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_source = \"enwiki-20230401\"\n",
    "# from factscore.call_llms import LLaMA3\n",
    "# lm = LLaMA3(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "from factscore.clm import LLaMA3\n",
    "lm = LLaMA3(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            cache_file=os.path.join(args.cache_dir, \"inst-llama3-8B.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc292ae-77c6-4e9a-95fc-2ce1116d0d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(input_file):\n",
    "    with open(input_file, \"r\", encoding = \"utf-8\") as f:\n",
    "        return f.readlines()\n",
    "\n",
    "def save_txt(data, output_file):\n",
    "    with open(output_file, \"w\", encoding = \"utf-8\") as writer:\n",
    "        writer.write(\"\\n\".join(data))\n",
    "\n",
    "# save_txt(factoolqa + felmwk, \"./data/labeled/topics.txt\")\n",
    "topics = read_txt(\"./data/labeled/topics.txt\")\n",
    "topic_dict = {'factool-qa': topics[:50], 'felm-wk': topics[50:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de41bf4b-f030-4b66-8c70-bf1c2e244962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 18:46:48,827\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 18:46:49 llm_engine.py:98] Initializing an LLM engine (v0.4.1) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 18:46:51 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 04-28 18:46:51 selector.py:28] Using FlashAttention backend.\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:51 utils.py:608] Found nccl from library /home/yuxiawang/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:52 selector.py:28] Using FlashAttention backend.\n",
      "INFO 04-28 18:46:52 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:52 pynccl_utils.py:43] vLLM is using nccl==2.18.1\n",
      "INFO 04-28 18:46:52 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "WARNING 04-28 18:46:52 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:52 utils.py:129] reading GPU P2P access cache from /home/yuxiawang/.config/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m WARNING 04-28 18:46:52 custom_all_reduce.py:74] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 04-28 18:46:53 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:53 weight_utils.py:193] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 18:46:54 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:55 model_runner.py:173] Loading model weights took 7.4829 GB\n",
      "INFO 04-28 18:46:56 ray_gpu_executor.py:217] # GPU blocks: 12270, # CPU blocks: 4096\n",
      "INFO 04-28 18:46:57 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-28 18:46:57 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:57 model_runner.py:976] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:46:57 model_runner.py:980] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerWrapper pid=197910)\u001b[0m INFO 04-28 18:47:01 model_runner.py:1057] Graph capturing finished in 4 secs.\n",
      "INFO 04-28 18:47:01 model_runner.py:1057] Graph capturing finished in 4 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.94it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.17it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.23it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 14.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.00it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.08s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.52it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.89it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.12it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.13it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 12.18it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 12.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.64it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.37it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.91it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.28it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  6.65it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.86it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.23it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.79it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 12.05it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 10.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.04it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.88it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.87it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.89it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.48it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.93it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 13.43it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.81it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.81it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.54it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.85it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.43it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.31it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.32it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.18it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.44it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.82it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.00s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.01it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 10.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.15it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.91it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 11.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.85it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.26it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.81it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.21s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.21it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.00it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.96it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.75it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.07it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00, 10.38it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.05it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.49it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.77it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.22it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.43it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.55it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.16it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.90it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.32it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.47it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.98it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.41it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.10it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail to retrieve related passages!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.24it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.39it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.40it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.49it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Processed prompts: 100%|██████████████████████████| 1/1 [00:00<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "decisions = []\n",
    "data = []\n",
    "\n",
    "for source in ['factool-qa', 'felm-wk']:\n",
    "    df1 = df[df['source'] == source]\n",
    "    # print(len(df1))\n",
    "    for k, v in df1.iterrows():\n",
    "        # print(k)\n",
    "        atomic_facts = v['claims']\n",
    "        topic = topics[k].strip()\n",
    "        \n",
    "\n",
    "        for atom in atomic_facts:\n",
    "            atom = atom.strip()\n",
    "            if lm:\n",
    "                try:\n",
    "                    passages = retrieval[knowledge_source].get_passages(topic, atom, k=5)\n",
    "                    # print(atom, passages)\n",
    "                except:\n",
    "                    # the topic is not in the wikipedia dump\n",
    "                    print(\"Fail to retrieve related passages!\")\n",
    "                    passages = [{\"title\": \"\", \"text\": \"\"}]\n",
    "                    \n",
    "                \n",
    "                definition = \"Answer the question about {} based on the given context.\\n\\n\".format(topic)\n",
    "                context = \"\"\n",
    "                for psg_idx, psg in enumerate(reversed(passages)):\n",
    "                    context += \"Title: {}\\nText: {}\\n\\n\".format(psg[\"title\"], psg[\"text\"].replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
    "                definition += context.strip()\n",
    "                if not definition[-1] in string.punctuation:\n",
    "                    definition += \".\"\n",
    "                prompt = \"{}\\n\\nInput: {} True or False?\\nOutput:\".format(definition.strip(), atom.strip())\n",
    "                # prompts.append(prompt)\n",
    "        \n",
    "                output = lm.generate(prompt)\n",
    "                \n",
    "                # when logits are unavailable\n",
    "                generated_answer = output[0].lower()\n",
    "                if \"true\" in generated_answer or \"false\" in generated_answer:\n",
    "                    if \"true\" in generated_answer and \"false\" not in generated_answer:\n",
    "                        is_supported = True\n",
    "                    elif \"false\" in generated_answer and \"true\" not in generated_answer:\n",
    "                        is_supported = False\n",
    "                    else:\n",
    "                        is_supported = generated_answer.index(\"true\") > generated_answer.index(\"false\")\n",
    "                else:\n",
    "                    is_supported = all([keyword not in generated_answer.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split() for keyword in [\"not\", \"cannot\", \"unknown\", \"information\"]])\n",
    "            \n",
    "            decisions.append({\"atom\": atom, \"is_supported\": is_supported})\n",
    "            data.append({\"source\": source, \"id\": k, \"atom\": atom, \"evidence\": passages, \n",
    "                         \"prompt\": prompt, \"llm_eval_response\": output[0], \"is_supported\": is_supported})\n",
    "            pd.DataFrame(data).to_json(\"factscore_evaluation.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c9ad4-0bf2-4f40-adae-c41bf55875d0",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5820890-b6d0-4fbf-9857-eda8d92bb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, f1_score, recall_score\n",
    "def eval_classification(y_true, y_pred, average=\"macro\"):\n",
    "    precision, recall, F1, support = precision_recall_fscore_support(y_true, y_pred, average=average)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": round(accuracy, 3),\n",
    "        \"precision\": round(precision, 3),\n",
    "        \"recall\": round(recall, 3),\n",
    "        \"F1\": round(F1, 3),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def eval_binary_classification(y_true, y_pred, pos_label=\"yes\"):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label=pos_label)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=pos_label)\n",
    "    F1 = f1_score(y_true, y_pred, pos_label=pos_label)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": round(accuracy, 2),\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"F1\": round(F1, 2),\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b5c75d9-8c67-4bf4-8dac-d5675350714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 & 0.58 & 0.68 & 0.31 & 0.59 & 0.4\n",
      "0.77 & 0.71 & 0.74 & 0.36 & 0.43 & 0.39\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"./data/labeled/Factbench.jsonl\", lines=True)\n",
    "gold_labels = {'factcheckgpt': [], 'factool-qa': [], 'felm-wk': [], 'halueval-dolly15k': []}\n",
    "for source in ['factcheckgpt', 'factool-qa', 'felm-wk', 'halueval-dolly15k']:        \n",
    "    t = df[df['source'] == source]\n",
    "    if source == 'halueval-dolly15k':\n",
    "        gold_labels[source] = list(t['response_label'])\n",
    "    else:\n",
    "        for l in t['claim_labels']:\n",
    "            gold_labels[source] += l\n",
    "\n",
    "# df = pd.read_json(\"./factscore_evaluation.jsonl\", lines=True)\n",
    "df = pd.read_json(\"./factscore_evaluation_chatgpt.jsonl\", lines=True)\n",
    "for source in ['factool-qa', 'felm-wk']:    \n",
    "    d1 = df[df['source'] == source]\n",
    "    # print(len(d1))\n",
    "    p = d1[\"is_supported\"]\n",
    "    v = gold_labels[source]\n",
    "    mt = eval_binary_classification(v, p, pos_label=True)\n",
    "    mf = eval_binary_classification(v, p, pos_label=False)\n",
    "    print(f\"{mt['precision']} & {mt['recall']} & {mt['F1']} & {mf['precision']} & {mf['recall']} & {mf['F1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a20aec-a674-416d-9917-7e217e8a3ccd",
   "metadata": {},
   "source": [
    "### FactScore with ChatGPT as Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a389975f-d541-4dc3-90b7-4b0cae50357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-05-02 16:43:40,000\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from factscore.call_llms import gpt_easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8effc56e-6313-4a97-af02-821c7f6592eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "decisions, data = [], []\n",
    "df = pd.read_json(\"./factscore_evaluation.jsonl\", lines=True)\n",
    "for source in ['factool-qa', 'felm-wk']:    \n",
    "    d1 = df[df['source'] == source]\n",
    "    print(len(d1))\n",
    "    for id, prompt in d1['prompt'].items():\n",
    "        output = gpt_easy(prompt, model=\"gpt-3.5-turbo-0125\", system_role=\"You are a helpful assistant.\")\n",
    "        # when logits are unavailable\n",
    "        generated_answer = output.lower()\n",
    "        if \"true\" in generated_answer or \"false\" in generated_answer:\n",
    "            if \"true\" in generated_answer and \"false\" not in generated_answer:\n",
    "                is_supported = True\n",
    "            elif \"false\" in generated_answer and \"true\" not in generated_answer:\n",
    "                is_supported = False\n",
    "            else:\n",
    "                is_supported = generated_answer.index(\"true\") > generated_answer.index(\"false\")\n",
    "        else:\n",
    "            is_supported = all([keyword not in generated_answer.lower().translate(str.maketrans(\"\", \"\", string.punctuation)).split() for keyword in [\"not\", \"cannot\", \"unknown\", \"information\"]])\n",
    "    \n",
    "        decisions.append(is_supported)\n",
    "        data.append({\"source\": source, \"id\": id, \n",
    "                     \"prompt\": prompt, \"llm_eval_response\": output, \"is_supported\": is_supported})\n",
    "        pd.DataFrame(data).to_json(\"factscore_evaluation_chatgpt.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ec6a2-a907-468e-a231-40139f6fa74c",
   "metadata": {},
   "source": [
    "### Generate Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2bf34333-752a-4daf-b80b-3c6b8f725583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States has the highest number of nuclear power plants in the world [{'title': 'Nuclear power by country', 'text': \"<s>Nuclear power by country Nuclear power plants operate in 32 countries and generate about a tenth of the world's electricity. Most are in Europe, North America, East Asia and South Asia. The United States is the largest producer of nuclear power, while France has the largest share of electricity generated by nuclear power, at about 70%. China has the fastest growing nuclear power programme with 16 new reactors under construction, followed by India, which has 8 under construction. Some countries operated nuclear reactors in the past but have no operating nuclear plants. Among them, Italy closed all of its nuclear stations by 1990 and nuclear power has since been discontinued because of the 1987 referendums. Kazakhstan is planning to reintroduce nuclear power in the future. Belarus began operating one unit of its first nuclear power plant in June 2021 and expects to bring the second unit into operation in 2022. Spain and Switzerland are currently operating nuclear power plants while planning nuclear power phase-outs. Germany will complete the shut down of its nuclear fleet in 2022 and any restart has been ruled out on technical grounds. }} Taiwan is considering a phase-out. Austria (Zwentendorf Nuclear Power Plant) and the Philippines (Bataan Nuclear Power Plant) never started to use their first nuclear plants that were completely built. Sweden and\"}, {'title': 'Nuclear power by country', 'text': ' supply as of 2021. Other countries have significant amounts of nuclear power generation capacity. By far the largest nuclear electricity producers are the United States with 771,638 GWh of nuclear electricity in 2021, followed by China with 383,205 GWh. As of August 2022, 438 reactors with a net capacity of 393,333 MWe are operational, and 57 reactors with net capacity of 58,858 MWe are under construction. Of the reactors under construction, 18 reactors with 18,526 MWe are in China and 8 reactors with a capacity of 6,028 MWe are in India.</s><s>See also. - List of commercial nuclear reactors - List of nuclear power stations - Nuclear energy policy by country - List of nuclear power accidents by country - List of countries by uranium reserves - World Nuclear Industry Status Report - Nuclear industry in Canada</s>'}, {'title': 'Nuclear power by country', 'text': \" Belgium originally had phase-out policies however they have now moved away from their original plans. The Philippines relaunched their nuclear programme on February 28, 2022 and may soon operate the mothballed Bataan Plant. Due to financial, political and technical reasons, Cuba, Libya and Poland never completed the construction of their first nuclear plants, and Australia, Azerbaijan, Georgia, Ghana, Ireland, Kuwait, Oman, Peru and Singapore never built their planned first nuclear plants. Some of these countries are still planning to introduce nuclear power. As of 2020, Poland is in advanced planning phase for 1.5 GW and plans to have up to 9 GW by 2040. Hong Kong has no nuclear power plants within its boundary, but imports 80% of the electricity generated from Daya Bay Nuclear Power Station located across the border, in which the power company of the territory holds stake. The government had also proposed to increase the share of nuclear energy to 50%. In 2021, Iraq declared it plans to build 8 nuclear reactors by 2030 to supply up to 25% electric power in the grid that suffers from shortages.</s><s>Overview. Of the 32 countries in which nuclear power plants operate, only France, Slovakia, Ukraine and Belgium use them as the source for a majority of the country's electricity\"}]\n"
     ]
    }
   ],
   "source": [
    "df1 = df[df['source'] == 'factool-qa']\n",
    "for k, v in df1.iterrows():\n",
    "    # print(v['prompt'], v['claims'])\n",
    "    # topic = v['prompt']\n",
    "    topic = \"Nuclear power by country\"\n",
    "    for atom in v['claims']:\n",
    "        passages = retrieval[knowledge_source].get_passages(topic, atom, k=5)\n",
    "        print(atom, passages)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "690df922-19f7-4d6a-b7c0-0098dc8e9abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which country or city has the maximum number of nuclear power plants? Wikipedia\n",
      "Nuclear power by country\n",
      "Who is the CEO of Twitter? Wikipedia\n",
      "Twitter - Wikipedia\n",
      "Is Jupiter more dense than Saturn? Wikipedia\n",
      "Saturn\n",
      "How many sons had eise eisinga in total? Wikipedia\n",
      "Eise Eisinga - Wikipedia\n",
      "How many times did Argentina win the FIFA world cup? Wikipedia\n",
      "FIFA World Cup - Wikipedia\n",
      "What is the fastest animal with wings and fur? Wikipedia\n",
      "Peregrine falcon - Wikipedia\n",
      "How many female US. Supreme Court justices have there been? Wikipedia\n",
      "List of justices of the Supreme Court of the United States\n",
      "Where did fortune cookies originate? Wikipedia\n",
      "Fortune cookie - Wikipedia\n",
      "What is the most valuable Non-fungible Token (NFT)? Wikipedia\n",
      "Non-fungible token - Wikipedia\n",
      "What is the most commonly used language in the world? Wikipedia\n",
      "English language - Wikipedia\n",
      "What is the smallest ocean in the world? Wikipedia\n",
      "Arctic Ocean - Wikipedia\n",
      "Who was the founder of Buddhism? Wikipedia\n",
      "The Buddha - Wikipedia\n",
      "Which planet in our solar system has the most moons? Wikipedia\n",
      "List of natural satellites - Wikipedia\n",
      "Who is the most decorated Olympian of all time? Wikipedia\n",
      "List of multiple Olympic medalists\n",
      "Who was the first African American woman to win the Nobel Prize in literature? Wikipedia\n",
      "List of black Nobel laureates - Wikipedia\n",
      "What is the name of the world's oldest living tree? Wikipedia\n",
      "List of longest-living organisms - Wikipedia\n",
      "What is the highest mountain in the solar system? Wikipedia\n",
      "Olympus Mons\n",
      "What is the name of the first woman to receive a Ph.D. in computer science? Wikipedia\n",
      "Timeline of women in computing - Wikipedia\n",
      "Who invented the light bulb? Wikipedia\n",
      "Edison light bulb - Wikipedia\n",
      "US incomes are less unequal than people in which countries? Wikipedia\n",
      "Income inequality in the United States\n",
      "Why do American trains have \"noses\" at the front of the locomotives but European, for example, don't? Wikipedia\n",
      "Why do American trains have 'noses' at the front of ...\n",
      "Why do veins appear blue? Wikipedia\n",
      "Vein - Simple English Wikipedia, the free encyclopedia\n",
      "Who are 2 black US presidents? Wikipedia\n",
      "List of African-American United States presidential and vice ...\n",
      "What is the name of the largest waterfall in the world? Wikipedia\n",
      "Waterfall - Wikipedia\n",
      "Which country is the largest producer of diamonds in the world? Wikipedia\n",
      "List of countries by diamond production\n",
      "Who was the longest-reigning monarch in British history? Wikipedia\n",
      "List of British monarchy records - Wikipedia\n",
      "What are some EU countries with big reserves of oil? Wikipedia\n",
      "List of countries by proven oil reserves\n",
      "All Mexicans are members of what Church? Wikipedia\n",
      "History of the Catholic Church in Mexico - Wikipedia\n",
      "Do Canadians work less hours than Mexicans? Wikipedia\n",
      "List of countries by average annual labor hours\n",
      "What is the reason behind NATO naming the Su-57 as the felon? Wikipedia\n",
      "NATO Code Name “FELON”: Russian Su-57 Gets Its ...\n",
      "In professional disc golf, in what years did Paige Pierce and Paul McBeth both win the world championship? Wikipedia\n",
      "Paige Pierce\n",
      "What was the 'New Populism'? Wikipedia\n",
      "Right-wing populism - Wikipedia\n",
      "Who signed the Louisiana Purchase? Wikipedia\n",
      "Louisiana Purchase - Wikipedia\n",
      "Does Mexico have a bigger problem with obesity than Ireland? Wikipedia\n",
      "List of countries by obesity rate\n",
      "What colors do dogs see? Wikipedia\n",
      "Color vision - Wikipedia\n",
      "Has Nigeria won a Nobel Prize? Wikipedia\n",
      "List of Nobel laureates by country\n",
      "How many ribs do humans have? Wikipedia\n",
      "Rib - Wikipedia\n",
      "Where did the Pilgrims first land? Wikipedia\n",
      "Plymouth Rock - Wikipedia\n",
      "What is the name of the oldest continuously inhabited city in the world? Wikipedia\n",
      "Which is the oldest continuously inhabited city in the world?\n",
      "Who was the first person to discover the existence of the bacterium responsible for causing tuberculosis? Wikipedia\n",
      "History | World TB Day | TB | CDC\n",
      "What is the most populated city in the world? Wikipedia\n",
      "List of largest cities - Wikipedia\n",
      "Which city is farther north, Oslo or Helsinki? Wikipedia\n",
      "List of northernmost settlements\n",
      "Assistant, what cities have served as the capital of California? Wikipedia\n",
      "California\n",
      "Who was the first person to successfully climb Mount Everest without the use of supplemental oxygen? Wikipedia\n",
      "Reinhold Messner - Wikipedia\n",
      "Who invented the first artificial heart? Wikipedia\n",
      "Artificial heart - Wikipedia\n",
      "What is the longest river in Africa? Wikipedia\n",
      "Nile - Simple English Wikipedia, the free encyclopedia\n",
      "What is the song of Tsinghua University？ Wikipedia\n",
      "Tsing Hua University\n",
      "Which country was the first to give women the right to vote? Wikipedia\n",
      "Timeline of women's suffrage\n",
      "Which existed first, the Aztec Empire or Oxford University? Wikipedia\n",
      "Aztec Empire\n",
      "What did SOS originally stand for? Wikipedia\n",
      "SOS\n"
     ]
    }
   ],
   "source": [
    "df1 = df[df['source'] == 'factool-qa']\n",
    "topics = []\n",
    "for k, v in df1.iterrows():\n",
    "    query = v['prompt'] + \" Wikipedia\"\n",
    "    topic = search_google_first_wikipedia_title(query)\n",
    "    # print(query)\n",
    "    # print(topic)\n",
    "    topics.append(topic)\n",
    "topics_new = [topic.replace(\"- Wikipedia\", \"\").strip() for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b94666-f21e-44e2-b216-6226f28a499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[df['source'] == 'felm-wk']\n",
    "topics = []\n",
    "for k, v in df2.iterrows():\n",
    "    query = v['prompt'] + \" Wikipedia\"\n",
    "    topic = search_google_first_wikipedia_title(query)\n",
    "    # print(query)\n",
    "    # print(topic)\n",
    "    topics.append(topic)\n",
    "\n",
    "topics_new = [topic.replace(\"- Wikipedia\", \"\").strip() for topic in topics]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
